{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0d5bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating  Year\n",
      "0                     Ship of Theseus      8  2012\n",
      "1                              Iruvar    8.4  1997\n",
      "2                     Kaagaz Ke Phool    7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1  2001\n",
      "4                     Pather Panchali    8.2  1955\n",
      "..                                ...    ...   ...\n",
      "95                        Apur Sansar    8.4  1959\n",
      "96                        Kanchivaram    8.2  2008\n",
      "97                    Monsoon Wedding    7.3  2001\n",
      "98                              Black    8.1  2005\n",
      "99                            Deewaar      8  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    names = []\n",
    "    ratings = []\n",
    "    years = []\n",
    "    \n",
    "    \n",
    "    movie_items = soup.find_all('div', class_='lister-item-content')\n",
    "    \n",
    "    \n",
    "    for movie in movie_items:\n",
    "        \n",
    "        name = movie.find('h3').find('a').text.strip()\n",
    "        names.append(name)\n",
    "        \n",
    "        \n",
    "        rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        \n",
    "        year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "        years.append(year)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Rating': ratings,\n",
    "        'Year': years\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "df = scrape_imdb_top_100_indian_movies(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145c9992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "posts = soup.find_all('div', class_='post')\n",
    "\n",
    "\n",
    "post_details = []\n",
    "\n",
    "\n",
    "for post in posts:\n",
    "    \n",
    "    heading = post.find('h2', class_='post-title').text.strip()\n",
    "    \n",
    "    \n",
    "    date = post.find('time', class_='post-date').text.strip()\n",
    "    \n",
    "    \n",
    "    content = post.find('div', class_='post-content').text.strip()\n",
    "    \n",
    "    \n",
    "    likes = post.find('div', class_='post-likes')['data-likes']\n",
    "    \n",
    "    \n",
    "    video_link = post.find('iframe', class_='youtube-video')['src']\n",
    "    \n",
    "    post_details.append({\n",
    "        'Heading': heading,\n",
    "        'Date': date,\n",
    "        'Content': content,\n",
    "        'Likes': likes,\n",
    "        'YouTube Link': video_link\n",
    "    })\n",
    "\n",
    "\n",
    "post_details_json = json.dumps(post_details, indent=4)\n",
    "\n",
    "\n",
    "print(post_details_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e2e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, EMI, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_house_details(localities):\n",
    "    \n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "\n",
    "    for locality in localities:\n",
    "        \n",
    "        url = f'https://www.nobroker.in/property/sale/{locality.lower()}/Bangalore?searchParam=W3sibGF0IjoxMi45MjU5ODk1LCJsb24iOjc3LjI5MTYxODgsInBsYWNlSWQiOiJDaElKSFkyYnZiWlMwcnl6X3RPNnZGUUhkQVwiLF9pZCI6WzEyODQ3NzVdXX0=&radius=2.0'\n",
    "\n",
    "        \n",
    "        response = requests.get(url)\n",
    "\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            \n",
    "            house_containers = soup.find_all('div', class_='card')\n",
    "\n",
    "            \n",
    "            for container in house_containers:\n",
    "                \n",
    "                title = container.find('h2', class_='heading-6 font-semi-bold nb__1AShY').text.strip()\n",
    "                location = container.find('div', class_='nb__2CMjv').text.strip()\n",
    "                area = container.find('div', class_='nb__3oNyC').text.strip()\n",
    "                emi = container.find('div', class_='font-semi-bold heading-6', text='â¹/month').find_next('div').text.strip()\n",
    "                price = container.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "\n",
    "                \n",
    "                titles.append(title)\n",
    "                locations.append(location)\n",
    "                areas.append(area)\n",
    "                emis.append(emi)\n",
    "                prices.append(price)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'Title': titles, 'Location': locations, 'Area': areas, 'EMI': emis, 'Price': prices})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "\n",
    "house_details_df = scrape_house_details(localities)\n",
    "\n",
    "\n",
    "print(house_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9b6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "        product_containers = soup.find_all('div', class_='productCard')\n",
    "\n",
    "        \n",
    "        product_names = []\n",
    "        product_prices = []\n",
    "        image_urls = []\n",
    "\n",
    "        \n",
    "        for container in product_containers[:10]:  \n",
    "            \n",
    "            product_name = container.find('p', class_='name').text.strip()\n",
    "            product_names.append(product_name)\n",
    "\n",
    "            \n",
    "            product_price = container.find('span', class_='discountedPrice').text.strip()\n",
    "            product_prices.append(product_price)\n",
    "\n",
    "            \n",
    "            image_url = container.find('img')['src']\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        \n",
    "        product_details = {\n",
    "            'Product Name': product_names,\n",
    "            'Price': product_prices,\n",
    "            'Image URL': image_urls\n",
    "        }\n",
    "\n",
    "        return product_details\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from Bewakoof website.')\n",
    "        return None\n",
    "\n",
    "\n",
    "bewakoof_url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "\n",
    "product_details = scrape_product_details(bewakoof_url)\n",
    "\n",
    "\n",
    "for index, (name, price, url) in enumerate(zip(product_details['Product Name'], product_details['Price'], product_details['Image URL']), start=1):\n",
    "    print(f'Product {index}:')\n",
    "    print(f'Name: {name}')\n",
    "    print(f'Price: {price}')\n",
    "    print(f'Image URL: {url}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c464496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fd1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading: How 26-year-old balances public office, working at Google and getting her MBA\n",
      "Date: 12 Min Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/meet-bushra-amiwala-one-of-the-first-gen-z-women-to-hold-public-office-in-the-us.html\n",
      "--------------------\n",
      "Heading: Here are the 3 key things we're most focused on in the stock market this week\n",
      "Date: 12 Min Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/the-3-things-we-are-most-focused-on-in-the-stock-market-this-week.html\n",
      "--------------------\n",
      "Heading: Pet insurance didn't feel worth it—until my cat needed a $3,000 surgery\n",
      "Date: 12 Min Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/is-pet-insurance-worth-it.html\n",
      "--------------------\n",
      "Heading: How this mission-driven chocolate company makes $162 million a year\n",
      "Date: 12 Min Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/how-this-mission-driven-chocolate-company-makes-162-million-a-year.html\n",
      "--------------------\n",
      "Heading: Nvidia CEO: Smart people struggle with these 2 traits—but they saved my company\n",
      "Date: 57 Min Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/nvidia-ceo-these-soft-skills-saved-my-2-trillion-company.html\n",
      "--------------------\n",
      "Heading: Generative AI 'FOMO' is driving tech heavyweights to invest billions in startups\n",
      "Date: 1 Hour Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html\n",
      "--------------------\n",
      "Heading: Decluttering tips from professional organizers that save time, money and stress\n",
      "Date: 1 Hour Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/decluttering-tips-from-professional-organizers.html\n",
      "--------------------\n",
      "Heading: As new vehicles become more like computers, what car shoppers need to know\n",
      "Date: 1 Hour Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/as-new-vehicles-become-more-like-computers-what-car-shoppers-need-to-know.html\n",
      "--------------------\n",
      "Heading: How Maersk grew its shipping empire and how it’s evolving\n",
      "Date: 1 Hour Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/how-maersk-grew-its-shipping-empire-and-how-its-evolving.html\n",
      "--------------------\n",
      "Heading: Buy these 5 stocks with upside potential as April kicks off, Goldman Sachs says\n",
      "Date: 2 Hours Ago\n",
      "News Link: https://www.cnbc.com/2024/03/30/goldman-sachs-says-buy-these-stocks-in-april.html\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "articles = soup.find_all('div', class_='LatestNews-container')\n",
    "\n",
    "\n",
    "news_details = []\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    \n",
    "    heading = article.find('a', class_='LatestNews-headline').text.strip()\n",
    "    \n",
    "    \n",
    "    date = article.find('time', class_='LatestNews-timestamp').text.strip()\n",
    "    \n",
    "    \n",
    "    link = article.find('a', class_='LatestNews-headline')['href']\n",
    "    \n",
    "    news_details.append({\n",
    "        'Heading': heading,\n",
    "        'Date': date,\n",
    "        'News Link': link\n",
    "    })\n",
    "\n",
    "\n",
    "for detail in news_details[:10]:  \n",
    "    print(f\"Heading: {detail['Heading']}\")\n",
    "    print(f\"Date: {detail['Date']}\")\n",
    "    print(f\"News Link: {detail['News Link']}\")\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d278ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data from KeAi Publishing.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print('Successfully fetched data from KeAi Publishing.')\n",
    "\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "        article_containers = soup.find_all('div', class_='articles-title')\n",
    "\n",
    "        \n",
    "        paper_titles = []\n",
    "        dates = []\n",
    "        authors = []\n",
    "\n",
    "        \n",
    "        for container in article_containers:\n",
    "            \n",
    "            paper_title = container.find('h3', class_='h2').text.strip()\n",
    "            paper_titles.append(paper_title)\n",
    "\n",
    "            \n",
    "            date = container.find('div', class_='date').text.strip()\n",
    "            dates.append(date)\n",
    "\n",
    "            \n",
    "            author = container.find('div', class_='authors').text.strip()\n",
    "            authors.append(author)\n",
    "\n",
    "        \n",
    "        article_data = {\n",
    "            'Paper Title': paper_titles,\n",
    "            'Date': dates,\n",
    "            'Author': authors\n",
    "        }\n",
    "\n",
    "        return article_data\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from KeAi Publishing. Status code:', response.status_code)\n",
    "        return None\n",
    "\n",
    "\n",
    "most_downloaded_articles_url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "\n",
    "most_downloaded_articles = scrape_most_downloaded_articles(most_downloaded_articles_url)\n",
    "\n",
    "if most_downloaded_articles:\n",
    "    \n",
    "    for index, (title, date, author) in enumerate(zip(most_downloaded_articles['Paper Title'], most_downloaded_articles['Date'], most_downloaded_articles['Author']), start=1):\n",
    "        print(f'Article {index}:')\n",
    "        print(f'Paper Title: {title}')\n",
    "        print(f'Date: {date}')\n",
    "        print(f'Author: {author}')\n",
    "        print()\n",
    "else:\n",
    "    print('No data to display. Check the code and try again.')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e143a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
